{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from scripts.reddit import scrape_political\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import display \n",
    "load_dotenv()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import combinations\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "conn = psycopg2.connect(\"postgres://postgres:postgres@127.0.0.1:5432/pubpol\")\n",
    "\n",
    "blacklist = ['www.reddit.com', 'redd.it', 'i.redd.it', 'v.redd.it', 'youtube.com',\n",
    "             'youtu.be', 'i.imgur.com', 'imgur.com', 'discord.gg', 'parler.com',\n",
    "             'google.com', 't.co', 'jssocial.pw', 'magaimg.net', \n",
    "             'streamable.com', 'pic8.co', 'kek.gg', 'www.youtube.com',\n",
    "             'gfycat.com', 'memefly.me', 'vgy.me', 'imgoat.com', \n",
    "             'twitter.com', 'vimeo.com', 'soundcloud.com',\n",
    "             'mega.nz']\n",
    "             \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we have different political beliefs?\n",
    "\n",
    "According to rational choice theory, people act *rationally* to maximize their happiness, according to some *utility function.* People tend to take this as a fact rather than a model, which leads to a lot of tonguewagging about voters acting against their own self-interest, and the subsequent inference that \"voters who disagree with me are stupid.\"\n",
    "\n",
    "We can expand this idea to make a more specific statement: \n",
    "* IF people are rational\n",
    "* AND they have the same self-interest\n",
    "* AND they operate from the same information\n",
    "* AND they interpret this information through the same set of experiences and values\n",
    "* THEN they will make the same decisions.  \n",
    "\n",
    "I'd contend that none of these conditions are met in practice. In this project, I'm specifically looking at the condition, **People operate from the same information** using the population **users of political subreddits**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What constitutes 'information'?\n",
    "\n",
    "In this context, I am using 'information' to mean description of facts as presented by a publication. A collection of stories creates a narrative. There are three ways that a publication can create a narrative that differs from the *true* state of things:\n",
    "\n",
    "1. Create false information and present it as true.\n",
    "2. Selectively report true information \n",
    "3. Contextualize information with other facts or opinion.\n",
    "\n",
    "This project is not going to exhaustively look at any of these, but it will touch on points 2 and 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do political subreddits use the same news?\n",
    "\n",
    "I'm going to look at the domains linked by political subreddits to get some of evidence of the idea that people are really just looking at completely different news sources. \n",
    "\n",
    "To get data, I accessed the reddit API on multiple days, and got information about the \"hot 1000\" posts for each subreddit of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_political()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What subreddits post from similar domains?\n",
    "\n",
    "We can look at this question in two different ways - using raw post counts, and adjusting them by the upvote scores. Adjusting by the upvote scores tells us, \"if we were to go browse that subreddit, what domains would we see most?\"\n",
    "\n",
    "It turns out that (self described) conservatives and republicans use similar sites, liberals/progressives/democrats use similar sites, and libertarians have their own sites. \n",
    "\n",
    "The anarchocapitalist subreddit seems to almost exclusively post memes and videos, so it's not worth including here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = ['politics', 'democrats', 'liberal', 'neoliberal', 'progressive',                   \n",
    "         'libertarian','conservative', 'conservatives','republican',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "(select \n",
    "    domain, lower(subreddit) as subreddit, count(1) as count\n",
    "from reddit\n",
    "group by domain, subreddit)\n",
    "'''\n",
    "count_table = pd.read_sql(query, conn).pivot(index='domain', columns = 'subreddit', values = 'count')\n",
    "count_table = count_table.loc[set(count_table.index) - set(blacklist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select lower(domain) as domain, lower(subreddit) as subreddit, sum(score) as score from\n",
    "(select domain, subreddit, 1.0 * score / sum(score) over (partition by subreddit) as score\n",
    "from reddit\n",
    "where domain not in ({})) a\n",
    "group by domain, subreddit\n",
    "'''.format(\", \".join([\"'\" + b + \"'\" for b in blacklist]))\n",
    "\n",
    "scores = pd.read_sql(query, conn)\n",
    "score_table = scores.pivot(index='domain', columns = 'subreddit', values = 'score')\n",
    "\n",
    "score_table_inverse = scores.pivot(index='subreddit', columns = 'domain', values = 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sti_normalized = score_table_inverse.loc[subs].fillna(0).apply(lambda x: x/max(x), 1)\n",
    "sti_normalized = sti_normalized.drop(blacklist, 1, errors = 'ignore')\n",
    "\n",
    "domains_to_plot = list(sti_normalized.loc[subs].sum().sort_values(ascending=False)[:30].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_mapping = {'thehill.com': 'center',\n",
    " 'babylonbee.com': 'satire',\n",
    " 'thefederalist.com': 'right, questionable',\n",
    " 'thinkprogress.org': 'left, questionable',\n",
    " 'nypost.com': 'right-center, questionable',\n",
    " 'talkingpointsmemo.com': 'left',\n",
    " 'lawandcrime.com': 'left-center',\n",
    " 'apnews.com': 'center',\n",
    " 'dailycaller.com': 'right, questionable',\n",
    " 'slate.com': 'left',\n",
    " 'reason.com': 'right-center',\n",
    " 'nymag.com': 'left',\n",
    " 'theweek.com': 'left',\n",
    " 'pjmedia.com': 'right-extreme, questionable',\n",
    " 'townhall.com': 'right-extreme, questionable',\n",
    " 'deadstate.org': 'left',\n",
    " 'justthenews.com': 'right, questionable',\n",
    " 'newrepublic.com': 'left',\n",
    " 'hotair.com': 'right',\n",
    " 'www.washingtonpost.com': 'left-center',\n",
    " 'politicaldig.com': 'left, questionable',\n",
    " 'redstate.com': 'right, questionable',\n",
    " 'crooksandliars.com': 'left',\n",
    " 'twitchy.com': 'right, questionable',\n",
    " 'newsmaven.io': 'unknown',\n",
    " 'amgreatness.com': 'right, questionable',\n",
    " 'www.breitbart.com': 'right-extreme, questionable',\n",
    " 'legalinsurrection.com': 'right',\n",
    " 'www.nytimes.com': 'left-center',\n",
    " 'prospect.org': 'left-center',\n",
    " 'spectator.org': 'right, questionable',\n",
    " 'issuesinsights.com': 'right, questionable'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = sti_normalized.loc[subs,domains_to_plot].dropna(how='all').fillna(0).corr()\n",
    "data_to_plot.index = [\": \".join([item, domain_mapping[item]]) for item in data_to_plot.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_norm = score_table.fillna(0).apply(lambda x: x/sum(x))\n",
    "scores_norm = scores_norm[scores_norm.sum(1)  >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_table.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_table[chi_sq_table.sum(1)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "query = '''\n",
    "select lower(domain) as domain, lower(subreddit) as subreddit, count(score) as score\n",
    "from reddit\n",
    "group by domain, subreddit\n",
    "'''.format(\", \".join([\"'\" + b + \"'\" for b in blacklist]))\n",
    "\n",
    "chi_sq_table = pd.read_sql(query, conn).pivot(index='domain', columns = 'subreddit', values = 'score').fillna(0).loc[domains_to_plot]\n",
    "chi_sq_table = chi_sq_table[['conservative', 'conservatives']]\n",
    "chi_sq_table = chi_sq_table[chi_sq_table.sum(1)>0]\n",
    "\n",
    "\n",
    "grand_total = chi_sq_table.sum().sum()\n",
    "chi_sq_expected = pd.DataFrame(\n",
    "    chi_sq_table.sum(1).to_numpy().reshape(-1,1) * chi_sq_table.sum(0).to_numpy().reshape(1,-1) / grand_total, \n",
    "    index = chi_sq_table.index,\n",
    "    columns = chi_sq_table.columns)\n",
    "\n",
    "\n",
    "\n",
    "for sub in chi_sq_table.columns:\n",
    "    print(sub, chisquare(chi_sq_table[sub], chi_sq_expected[sub]).pvalue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(scores_norm.politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.clustermap(\n",
    "    data=data_to_plot, \n",
    "    cmap=plt.cm.RdBu, \n",
    "    metric = 'correlation',\n",
    "    vmin=1, vmax = -1, yticklabels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sti_normalized.fillna(0).apply(lambda x: list(sti_normalized.columns[np.argsort(x)[-5:]]), 1, result_type = 'expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blacklisted sites and memes\n",
    "\n",
    "If I don't blacklist certain domains, the clustering looks very different! Certain subreddits are full of links to videos, twitter posts, and image memes. These posts are so common that they dominate the content similarity metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(\n",
    "    data=score_table.dropna(how='all').fillna(0).corr(), cmap=plt.cm.RdBu, \n",
    "    metric = 'correlation', vmin=1, vmax = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_mapping = {\n",
    "    \"worldpolitics\": \"blue\",\n",
    "    \"dsa\":\"blue\",\n",
    "    \"uspolitics\":\"blue\",\n",
    "    \"environment\":\"blue\",\n",
    "    \"democrats\":\"blue\",\n",
    "    \"progressive\":\"blue\",\n",
    "    \"liberal\":\"blue\",\n",
    "    \"politics\":\"blue\",\n",
    "    \"americanpolitics\":\"blue\",\n",
    "    \"neoliberal\":\"blue\",\n",
    "    \"worldnews\":\"blue\",\n",
    "    \"conservative\":\"red\",\n",
    "    \"conservatives\":\"red\",\n",
    "    \"republican\":'red'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select lower(domain) as domain, lower(subreddit) as subreddit, sum(score) as score from\n",
    "(select domain, subreddit, 1.0 * score / sum(score) over (partition by subreddit) as score\n",
    "from reddit\n",
    "where domain not in ({})) a\n",
    "group by domain, subreddit\n",
    "'''.format(\", \".join([\"'\" + b + \"'\" for b in blacklist]))\n",
    "\n",
    "top_domains = pd.read_sql(query, conn)\n",
    "top_domains['group'] = top_domains['subreddit'].replace(sub_mapping)\n",
    "top_domains = top_domains.groupby(['group', 'domain']).sum()\n",
    "# top_domains = top_domains[top_domains.score > 100]\n",
    "top_domains = top_domains.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(top_domains[top_domains.group == 'blue'].sort_values('score', ascending = False)[:10].domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(top_domains[top_domains.group == 'red'].sort_values('score', ascending = False)[:10].domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(corpus, n):\n",
    "    tfidf_ngram = []\n",
    "    for i in range(1, n+1):\n",
    "        vectorizer = TfidfVectorizer(ngram_range = (i,i), max_df=0.5, min_df=5, binary = True)\n",
    "        tfidf = vectorizer.fit_transform(corpus)\n",
    "        tfidf_ngram.append(pd.DataFrame(tfidf.toarray(), columns = vectorizer.get_feature_names()).mean())\n",
    "\n",
    "    return(tfidf_ngram)\n",
    "\n",
    "def contiguous_subset(values):\n",
    "    n = len(values)\n",
    "    breakpoints = [l for i in range(1, n) for l in list(combinations(range(1, n), i)) ]\n",
    "    groupings = []\n",
    "    for l in breakpoints:\n",
    "        grouping = []\n",
    "        breaks = [0] + list(l) + [n]\n",
    "        for idx in range(len(breaks)-1):\n",
    "            grouping.append(values[breaks[idx]:breaks[idx+1]])\n",
    "        groupings.append(grouping)\n",
    "    return(groupings)\n",
    "\n",
    "def ngram_importance(ngram, ngrams):\n",
    "    subsets = contiguous_subset(ngram.split())\n",
    "    bottom = []\n",
    "    for subset in subsets:\n",
    "        tfidf=[]\n",
    "        for gram in subset:\n",
    "            term = ' '.join(gram)\n",
    "            idx = len(gram)-1\n",
    "            tfidf.append(np.log(ngrams[idx][term]))\n",
    "        bottom.append(np.sum(tfidf))\n",
    "    top = ngrams[len(ngram.split())-1][ngram]\n",
    "    return(top/np.sum(np.exp(bottom)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select distinct url, title\n",
    "from reddit\n",
    "'''\n",
    "titles = pd.read_sql(query, conn)\n",
    "ngrams = n_grams(titles.title, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_to_replace = []\n",
    "for j in range(5):\n",
    "    for idx, item in enumerate(ngrams[j+1]):\n",
    "        ng = ngrams[j+1].index[idx]\n",
    "        if ngram_importance(ng, ngrams) > 25:\n",
    "            ngrams_to_replace.append(ng)\n",
    "ngrams_to_replace.reverse()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [(g, re.sub('\\s', '_', g)) for g in ngrams_to_replace]\n",
    "replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select distinct url, title\n",
    "from reddit\n",
    "where domain not in ({})\n",
    "'''.format(\", \".join([\"'\" + b + \"'\" for b in blacklist]))\n",
    "\n",
    "titles = pd.read_sql(query, conn)\n",
    "titles['title_orig'] = titles.title\n",
    "titles.title = titles.title.str.replace('([0-9]+,?)+\\.?[,0-9]* \\willion', '_number_')\n",
    "titles.title = titles.title.str.replace('(\\d+[./]){1,2}(\\d+)', '_date_')\n",
    "titles.title = titles.title.str.replace('[A-Za-z]{3} \\d+[â€“\\-./]\\d+', '_date_')\n",
    "titles.title = titles.title.str.replace('\\d{1,2}:\\d{2}', '_time_')\n",
    "titles.title = titles.title.str.replace('\\d{4}', '_year_')\n",
    "titles.title = titles.title.str.replace('([0-9]+,?)+\\.?[,0-9]*', '_number_')\n",
    "titles.title = titles.title.str.lower()\n",
    "\n",
    "for r in replacements:\n",
    "    titles.title = titles.title.str.replace(r[0], r[1])\n",
    "#     titles.title = titles.title.str.replace(r[0], \" \".join([r[1], r[0]]))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,1), max_df=0.5, min_df=5, binary = True, stop_words = 'english')\n",
    "tfidf = vectorizer.fit_transform(titles.title)\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns = vectorizer.get_feature_names())\n",
    "tfidf_df = titles.join(tfidf_df,lsuffix =\"_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select url, lower(subreddit) as subreddit, 1.0 * score / sum(score) over (partition by subreddit) as score\n",
    "from reddit\n",
    "where domain not in ({})\n",
    "'''.format(\", \".join([\"'\" + b + \"'\" for b in blacklist]))\n",
    "\n",
    "\n",
    "posts = pd.read_sql(query, conn)\n",
    "posts_tfidf = posts.merge(tfidf_df, on='url')\n",
    "posts_tfidf.iloc[:, 6:] = posts_tfidf.iloc[:, 6:].multiply(posts_tfidf['score'], axis=0)\n",
    "\n",
    "top_words_subreddit = posts_tfidf.drop(['url', 'score', 'title', 'title_orig'], 1).groupby('subreddit').sum()\n",
    "top_10_subreddit = pd.DataFrame(top_words_subreddit.apply(lambda x: top_words_subreddit.columns[np.argsort(x)[-10:]].to_list(), axis=1).to_list(), index=top_words_subreddit.index)\n",
    "display(top_10_subreddit.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select subreddit, url, lower(domain) as domain, 1.0 * score / sum(score) over (partition by subreddit) as score\n",
    "from reddit\n",
    "where domain in ('thehill.com', 'apnews.com')\n",
    "'''\n",
    "\n",
    "posts = pd.read_sql(query, conn)\n",
    "posts['sub_group'] = posts.subreddit.replace(sub_mapping)\n",
    "posts = posts[(posts.sub_group == 'red') | (posts.sub_group == 'blue')]\n",
    "posts_tfidf = posts.merge(tfidf_df, on='url')\n",
    "posts_tfidf.iloc[:, 7:] = posts_tfidf.iloc[:, 7:].multiply(posts_tfidf['score'], axis=0)\n",
    "\n",
    "top_words_group = posts_tfidf.drop(['url', 'score', 'title', 'title_orig'], 1).groupby('sub_group').sum()\n",
    "\n",
    "top_words_group = pd.DataFrame(top_words_group.apply(lambda x: top_words_group.columns[np.argsort(x)[-10:]].to_list(), axis=1).to_list(), index=top_words_group.index)\n",
    "\n",
    "top_words_group.swapaxes(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_grouped = top_words_subreddit.reset_index()\n",
    "top_words_grouped['subreddit_group'] = top_words_grouped.subreddit.replace(sub_mapping)\n",
    "top_words_grouped = top_words_grouped.groupby('subreddit_group').mean()\n",
    "top_words_grouped = top_words_grouped.loc[['blue', 'red']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_words_grouped['susan_collins'].sort_values(ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((top_words_group.loc['red'] - top_words_group.loc['blue']).sort_values()[-30:])\n",
    "print((top_words_group.loc['red'] - top_words_group.loc['blue']).sort_values()[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_grouped.loc['red'][top_words_grouped.loc['red']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_grouped.loc['blue'][top_words_grouped.loc['blue']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_words['riot'].sort_values(ascending = False)[0:5])\n",
    "print(top_words['protest'].sort_values(ascending = False)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_words['proud_boys'].sort_values(ascending = False)[0:5])\n",
    "print(top_words['black_lives_matter'].sort_values(ascending = False)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top_words['benghazi'].sort_values(ascending = False)[0:5])\n",
    "print(top_words['hunter_biden'].sort_values(ascending = False)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(top_words.loc['conservative'] - top_words.loc['liberal']).sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(top_words.loc['conservative'] - top_words.loc['liberal']).sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(top_words.loc['republican'] - top_words.loc['democrats']).sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(top_words.loc['republican'] - top_words.loc['democrats']).sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['google_api_key']\n",
    "\n",
    "def knowledge(query):\n",
    "    endpoint = 'https://kgsearch.googleapis.com/v1/entities:search?'\n",
    "    url = '{}query={}&key={}'.format(endpoint,query,key)\n",
    "    return(json.loads(requests.get(url).content))\n",
    "\n",
    "def parse_knowledge(knowledge):\n",
    "    return([item['result']['@type'] for item in knowledge['itemListElement']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knowledge('riots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item['result']['@type'] for item in json.loads(result)['itemListElement']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(result)['itemListElement'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knowledge(ngrams_to_replace[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_knowledge(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "know_ngram = []\n",
    "for idx, ngram in enumerate(ngrams_to_replace):\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "    know_ngram.append(knowledge(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "know_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, k in enumerate(know_ngram):\n",
    "    print(ngrams_to_replace[idx], parse_knowledge(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_to_replace[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datasci]",
   "language": "python",
   "name": "conda-env-datasci-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
